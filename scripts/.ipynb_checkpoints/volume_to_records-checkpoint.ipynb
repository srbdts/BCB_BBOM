{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5dbbb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split(lines):\n",
    "    pages = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"\f",
    "\"):\n",
    "            pages.append(current)\n",
    "            current = [line]\n",
    "        else:\n",
    "            current.append(line)\n",
    "    if len(current) > 0:\n",
    "        pages.append(current)\n",
    "    return pages\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "08b990da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data\n",
    "\n",
    "ipf = open(\"../data/Volume-I-Tome-I.txt\",\"r\",encoding=\"utf-8\")\n",
    "lines = ipf.readlines()\n",
    "pages = split(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86c30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_roman_numeral(s):\n",
    "    \"\"\"Checks whether a string is a roman numeral\"\"\"\n",
    "    l = []\n",
    "    for c in s:\n",
    "        if c in [\"I\",\"X\",\"L\",\"V\"]:\n",
    "            l.append(True)\n",
    "        else:\n",
    "            l.append(False)\n",
    "    if all(l):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def find_lemmata(headers):\n",
    "    \"\"\"Extracts the names of the lemmata on a given page from the provided headers\"\"\"\n",
    "    primary_header = headers[0]\n",
    "    primary_header = primary_header.strip(\"\f",
    "\")\n",
    "    primary_header = primary_header.strip(\" \")\n",
    "    if vol_nr == \"IX\":\n",
    "        primary_header = primary_header.replace(\"\b\",\"                      \")\n",
    "    query = r\"\\s\\s\\s\\s+\"\n",
    "    ms = re.finditer(query,primary_header)\n",
    "    ms = list(ms)\n",
    "    if len(ms) == 2:\n",
    "        names = primary_header[ms[0].end(0):ms[1].start(0)].strip()\n",
    "        page_begin = primary_header[:ms[0].start(0)].strip()\n",
    "        page_end = primary_header[ms[1].end(0):].strip()\n",
    "    else:\n",
    "        opf_error.write(\"\\tError in header extraction\\n\")\n",
    "        opf_error.write(\"\\t\"+primary_header+\"\\n\")\n",
    "        return None\n",
    "\n",
    "    primary_header = primary_header.strip()\n",
    "    parts = [part for part in primary_header.split(\" \") if len(part)>0]\n",
    "    if not page_begin.isnumeric():\n",
    "        page_begin = \"OCR_ERROR\"\n",
    "    if not page_end.isnumeric():\n",
    "        if not page_begin == \"OCR_ERROR\":\n",
    "            page_end = str(int(page_begin) + 1)\n",
    "        else:\n",
    "            page_end = \"OCR_ERROR\"\n",
    "    if page_begin == \"OCR_ERROR\":\n",
    "        if not page_end == \"OCR_ERROR\":\n",
    "            page_begin = str(int(page_end)-1)\n",
    "\n",
    "\n",
    "    if \"窶能" in names:\n",
    "        names = names.replace(\"窶能",\"-\")\n",
    "    if \"-\" in names:\n",
    "        names = names.replace(\"-\",\"-\")\n",
    "    if \"-\" in names:\n",
    "        names = names.replace(\"-\",\"-\")\n",
    "    if \"窶能" in names:\n",
    "        names = names.replace(\"窶能",\"-\")\n",
    "    name_sep = \"-\"\n",
    "        \n",
    "    if vol_nr == \"IX\":\n",
    "        all_names = [name.strip().upper() for name in names.split(name_sep) if len(name.strip())>1]\n",
    "    else:\n",
    "        all_names = [name.strip() for name in names.split(name_sep) if len(name.strip())>1]\n",
    "    \n",
    "    if len(headers)>1:\n",
    "        for header in headers[1:]:\n",
    "            extra_names = header.strip().strip(\" \")\n",
    "            if vol_nr == \"IX\":\n",
    "                all_names.extend([name.strip().upper() for name in extra_names.split(name_sep) if (len(name.strip())>1 and not name.strip().isnumeric())])\n",
    "            else:\n",
    "                all_names.extend([name.strip() for name in extra_names.split(name_sep) if (len(name.strip())>1 and not name.strip().isnumeric())])\n",
    "    return all_names,page_begin,page_end\n",
    "\n",
    "def find_column_separation(page):\n",
    "    \"\"\"Finds the (potentially flexible) cutoff point between the first and the second column on each line. This was necessary because the OCR engine transcribed the space between the two columns as a sequence of spaces.\"\"\"\n",
    "    ends_dic = {}\n",
    "    lines_dic = {}\n",
    "    query = r\"\\s\\s+\"\n",
    "    for i,line in enumerate(page):\n",
    "        ms = re.finditer(query,line)\n",
    "        ms = list(ms)\n",
    "        lines_dic[i] = ms\n",
    "        ends = [m.end(0) for m in ms]\n",
    "        for end in ends:\n",
    "            if not end in ends_dic:\n",
    "                ends_dic[end] = 1\n",
    "            else:\n",
    "                ends_dic[end]+=1\n",
    "    return ends_dic,lines_dic\n",
    "    \n",
    "def divide_in_columns(page,ends_dic,lines_dic):\n",
    "    \"\"\"Extract the text from the first and the second column based on the computed column sepatator\"\"\"\n",
    "    selected_boundaries = [boundary for boundary,count in ends_dic.items() if count/len(page)>0.1]\n",
    "    selected_boundaries = [sb for sb in selected_boundaries if sb>20]\n",
    "    min_b = min(selected_boundaries)\n",
    "\n",
    "    threshold = 0.1\n",
    "    kolom1 = []\n",
    "    kolom2 = []\n",
    "    errors = []\n",
    "    for i,line in enumerate(page):\n",
    "        divided = False\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        if len(line) < min_b:\n",
    "            divided = True\n",
    "            kolom1.append(line.strip())\n",
    "        else:\n",
    "            for m in lines_dic[i]:\n",
    "                if m.end(0) in selected_boundaries:\n",
    "                    kolom1.append(line[:m.end(0)].strip())\n",
    "                    kolom2.append(line[m.end(0):].strip())\n",
    "                    divided = True\n",
    "                    break\n",
    "        if not divided:\n",
    "            for b in selected_boundaries:\n",
    "                for m in lines_dic[i]:\n",
    "                    if m.start(0) <= b < m.end(0):\n",
    "                        kolom1.append(line[:m.end(0)].strip())\n",
    "                        kolom2.append(line[m.end(0):].strip())\n",
    "                        divided = True\n",
    "                        break\n",
    "                if divided:\n",
    "                    break\n",
    "\n",
    "        if not divided:\n",
    "            for m in lines_dic[i]:\n",
    "                if (m.end(0)+1 in selected_boundaries or m.end(0)+2 in selected_boundaries):\n",
    "                    kolom1.append(line[:m.end(0)].strip())\n",
    "                    kolom2.append(line[m.end(0):].strip())\n",
    "                    divided = True\n",
    "                    break\n",
    "        if not divided:\n",
    "            errors.append(line)\n",
    "    if not len(errors) == 0:\n",
    "        opf_error.write(\"\\tSegmentation errors: \\n\")\n",
    "        for e in errors:\n",
    "            opf_error.write(e)\n",
    "    return kolom1,kolom2\n",
    "\n",
    "def divide_in_records(text,names):\n",
    "    \"\"\"Divide the running texts into individual records based on the lemmata retrieved from the header\"\"\"\n",
    "    records = []\n",
    "    current_record = {\"name\":\"INHERIT\",\"text\":[]}\n",
    "    names_found = []\n",
    "    for line in text:\n",
    "        name_found = False\n",
    "        for name in names:\n",
    "            if name in line:\n",
    "                if len(current_record[\"text\"])>0:\n",
    "                    records.append(current_record)\n",
    "                current_record = {}\n",
    "                current_record[\"name\"] = name\n",
    "                current_record[\"text\"] = [line]\n",
    "                name_found = True\n",
    "                names_found.append(name)\n",
    "        if not name_found:\n",
    "            current_record[\"text\"].append(line)\n",
    "            words = line.split(\" \")\n",
    "            uppers = [w.isupper() for w in words]\n",
    "            potential_error = False\n",
    "            if any([w.isupper for w in words]):\n",
    "                for u,w in zip(uppers,words):\n",
    "                    if u == True and len(w)>2 and w.isalpha() and (not is_roman_numeral(w)):\n",
    "                        opf_error.write(\"\\tDetected uppercase word %s\\n\"%(w))\n",
    "                        potential_error = True\n",
    "            if potential_error:\n",
    "                opf_error.write(\"\\t**  Names to look for: %s\\n\"%(names))\n",
    "    if len(current_record[\"text\"]) > 0:\n",
    "        records.append(current_record)\n",
    "#\n",
    "    return records, names_found\n",
    "\n",
    "def merge_records(records1,records2,begin_page,end_page):\n",
    "    \"\"\"Merge the running texts from records that span several pages\"\"\"\n",
    "    records = []\n",
    "    for r in records1:\n",
    "        r[\"pages\"] = str(begin_page)\n",
    "        records.append(r)\n",
    "    if records2[0].get(\"name\") == \"INHERIT\":\n",
    "        records[-1][\"text\"].extend(records2[0][\"text\"])\n",
    "        records[-1][\"pages\"] = \"%s-%s\" % (begin_page,end_page)\n",
    "    else:\n",
    "        records2[0][\"pages\"] = str(end_page)\n",
    "        records.append(records2[0])\n",
    "    if len(records2)>1:\n",
    "        for r in records2[1:]:\n",
    "            r[\"pages\"] = str(end_page)\n",
    "            records.append(r)\n",
    "    return records\n",
    "\n",
    "def merge_text(record):\n",
    "    \"\"\"Add text to a record (this is needed to get rid of end-of-line hyphens)\"\"\"\n",
    "    all_text = \"\"\n",
    "    for line in record.get(\"text\"):\n",
    "        if len(line) == 0:\n",
    "            all_text += \"\\n\"\n",
    "        elif line[-1] == \"-\":\n",
    "            all_text += line[:-1]\n",
    "        else:\n",
    "            all_text += line + \" \"\n",
    "    record[\"text\"] = all_text\n",
    "    return record\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dab876eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is set to the first and last page with actual records in each volume\n",
    "\n",
    "vol_nr = \"IX\"\n",
    "BEGIN = 8\n",
    "END = 228\n",
    "#SKIP = [215]\n",
    "#II_BEGIN = 5\n",
    "#II_END = 502\n",
    "#III_BEGIN = 19\n",
    "#III_END = 490\n",
    "#IV_BEGIN = 23\n",
    "#IV_END = 507\n",
    "#V_BEGIN = 21\n",
    "#V_END = 472\n",
    "#VI_BEGIN = 22\n",
    "#VI_END = 591\n",
    "#VIIa_BEGIN = 4\n",
    "#VIIa_END = 252\n",
    "#VIIb_BEGIN = 4\n",
    "#VIIb_END = 204\n",
    "#VIIc_BEGIN = 16\n",
    "#VIIc_END = 208\n",
    "#VIII_BEGIN = 6\n",
    "#VIII_END = 243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8d3866b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8\n",
      "Processing 9\n",
      "Processing 10\n",
      "Processing 11\n",
      "Processing 12\n",
      "Processing 13\n",
      "Processing 14\n",
      "Processing 15\n",
      "Processing 16\n",
      "Processing 17\n",
      "Processing 18\n",
      "Processing 19\n",
      "Processing 20\n",
      "Processing 21\n",
      "Processing 22\n",
      "Processing 23\n",
      "Processing 24\n",
      "Processing 25\n",
      "Processing 26\n",
      "Processing 27\n",
      "Processing 28\n",
      "Processing 29\n",
      "Processing 30\n",
      "Processing 31\n",
      "Processing 32\n",
      "Processing 33\n",
      "Processing 34\n",
      "Processing 35\n",
      "Processing 36\n",
      "Processing 37\n",
      "Processing 38\n",
      "Processing 39\n",
      "Processing 40\n",
      "Processing 41\n",
      "Processing 42\n",
      "Processing 43\n",
      "Processing 44\n",
      "Processing 45\n",
      "Processing 46\n",
      "Processing 47\n",
      "Processing 48\n",
      "Processing 49\n",
      "Processing 50\n",
      "Processing 51\n",
      "Processing 52\n",
      "Processing 53\n",
      "Processing 54\n",
      "Processing 55\n",
      "Processing 56\n",
      "Processing 57\n",
      "Processing 58\n",
      "Processing 59\n",
      "Processing 60\n",
      "Processing 61\n",
      "Processing 62\n",
      "Processing 63\n",
      "Processing 64\n",
      "Processing 65\n",
      "Processing 66\n",
      "Processing 67\n",
      "Processing 68\n",
      "Processing 69\n",
      "Processing 70\n",
      "Processing 71\n",
      "Processing 72\n",
      "Processing 73\n",
      "Processing 74\n",
      "Processing 75\n",
      "Processing 76\n",
      "Processing 77\n",
      "Processing 78\n",
      "Processing 79\n",
      "Processing 80\n",
      "Processing 81\n",
      "Processing 82\n",
      "Processing 83\n",
      "Processing 84\n",
      "Processing 85\n",
      "Processing 86\n",
      "Processing 87\n",
      "Processing 88\n",
      "Processing 89\n",
      "Processing 90\n",
      "Processing 91\n",
      "Processing 92\n",
      "Processing 93\n",
      "Processing 94\n",
      "Processing 95\n",
      "Processing 96\n",
      "Processing 97\n",
      "Processing 98\n",
      "Processing 99\n",
      "Processing 100\n",
      "Processing 101\n",
      "Processing 102\n",
      "Processing 103\n",
      "Processing 104\n",
      "Processing 105\n",
      "Processing 106\n",
      "Processing 107\n",
      "Processing 108\n",
      "Processing 109\n",
      "Processing 110\n",
      "Processing 111\n",
      "Processing 112\n",
      "Processing 113\n",
      "Processing 114\n",
      "Processing 115\n",
      "Processing 116\n",
      "Processing 117\n",
      "Processing 118\n",
      "Processing 119\n",
      "Processing 120\n",
      "Processing 121\n",
      "Processing 122\n",
      "Processing 123\n",
      "Processing 124\n",
      "Processing 125\n",
      "Processing 126\n",
      "Processing 127\n",
      "Processing 128\n",
      "Processing 129\n",
      "Processing 130\n",
      "Processing 131\n",
      "Processing 132\n",
      "Processing 133\n",
      "Processing 134\n",
      "Processing 135\n",
      "Processing 136\n",
      "Processing 137\n",
      "Processing 138\n",
      "Processing 139\n",
      "Processing 140\n",
      "Processing 141\n",
      "Processing 142\n",
      "Processing 143\n",
      "Processing 144\n",
      "Processing 145\n",
      "Processing 146\n",
      "Processing 147\n",
      "Processing 148\n",
      "Processing 149\n",
      "Processing 150\n",
      "Processing 151\n",
      "Processing 152\n",
      "Processing 153\n",
      "Processing 154\n",
      "Processing 155\n",
      "Processing 156\n",
      "Processing 157\n",
      "Processing 158\n",
      "Processing 159\n",
      "Processing 160\n",
      "Processing 161\n",
      "Processing 162\n",
      "Processing 163\n",
      "Processing 164\n",
      "Processing 165\n",
      "Processing 166\n",
      "Processing 167\n",
      "Processing 168\n",
      "Processing 169\n",
      "Processing 170\n",
      "Processing 171\n",
      "Processing 172\n",
      "Processing 173\n",
      "Processing 174\n",
      "Processing 175\n",
      "Processing 176\n",
      "Processing 177\n",
      "Processing 178\n",
      "Processing 179\n",
      "Processing 180\n",
      "Processing 181\n",
      "Processing 182\n",
      "Processing 183\n",
      "Processing 184\n",
      "Processing 185\n",
      "Processing 186\n",
      "Processing 187\n",
      "Processing 188\n",
      "Processing 189\n",
      "Processing 190\n",
      "Processing 191\n",
      "Processing 192\n",
      "Processing 193\n",
      "Processing 194\n",
      "Processing 195\n",
      "Processing 196\n",
      "Processing 197\n",
      "Processing 198\n",
      "Processing 199\n",
      "Processing 200\n",
      "Processing 201\n",
      "Processing 202\n",
      "Processing 203\n",
      "Processing 204\n",
      "Processing 205\n",
      "Processing 206\n",
      "Processing 207\n",
      "Processing 208\n",
      "Processing 209\n",
      "Processing 210\n",
      "Processing 211\n",
      "Processing 212\n",
      "Processing 213\n",
      "Processing 214\n",
      "Processing 215\n",
      "Processing 216\n",
      "Processing 217\n",
      "Processing 218\n",
      "Processing 219\n",
      "Processing 220\n",
      "Processing 221\n",
      "Processing 222\n",
      "Processing 223\n",
      "Processing 224\n",
      "Processing 225\n",
      "Processing 226\n",
      "Processing 227\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# errors are written to a separate file so they can be manually checked/corrected\n",
    "opf_error = open(\"processing_errors_vol_%s.txt\"% vol_nr,\"w\",encoding=\"utf-8\")\n",
    "pages = []\n",
    "\n",
    "for i in range(BEGIN,END):\n",
    "    ipf = open(\"verbeterd/vol_%s/%s.txt\" % (vol_nr,i),\"r\",encoding=\"utf-8\")\n",
    "    pages.append(ipf.readlines())\n",
    "    \n",
    "all_records = []\n",
    "all_records_clean = []\n",
    "\n",
    "begin_index = 0\n",
    "#end_index = \n",
    "\n",
    "last_name_of_previous_page = \"INHERIT\"\n",
    "\n",
    "for i,page in enumerate(pages[begin_index:]):\n",
    "    page_i = begin_index+i+1\n",
    "    opf_error.write(\"Processing %s\\n\" % (page_i+BEGIN-1))\n",
    "    print(\"Processing %s\"%(page_i+BEGIN-1))\n",
    "    if page_i+BEGIN-1 in SKIP:\n",
    "        continue\n",
    "        \n",
    "    # separate header from body \n",
    "    \n",
    "    cutoff_header = 0\n",
    "    headers = []\n",
    "    keep_looking = True\n",
    "    try:\n",
    "        while keep_looking:\n",
    "            if len(page[cutoff_header].strip()) == 0:\n",
    "                keep_looking = False\n",
    "            else:\n",
    "                headers.append(page[cutoff_header])\n",
    "                cutoff_header += 1\n",
    "    except IndexError:\n",
    "        opf_error.write(\"\\tNo whitespace between header and body in page %s\\n\"%(page_i+BEGIN-1))\n",
    "        continue\n",
    "\n",
    "    body_raw = page[cutoff_header:]\n",
    "    remove_whitespace = True\n",
    "    cutoff_body =  cutoff_header\n",
    "    while remove_whitespace:\n",
    "        if len(page[cutoff_body].strip()) > 0:\n",
    "            remove_whitespace = False\n",
    "        else:\n",
    "            cutoff_body += 1\n",
    "    body = page[cutoff_body:]\n",
    "    if vol_nr == \"IX\":\n",
    "        body = body[:-1]\n",
    "\n",
    "    # try and extract names of lemmata from the header         \n",
    "    try:\n",
    "        names,page_begin,page_end = find_lemmata(headers)\n",
    "    except TypeError:\n",
    "        continue\n",
    "        \n",
    "    # detect the column boundary between the first and the second column on this page and divide the page into columns\n",
    "    ends_dic,lines_dic = find_column_separatrion(body)\n",
    "    kolom1,kolom2 = divide_in_columns(body,ends_dic,lines_dic)\n",
    "    \n",
    "    # divide each column into records\n",
    "    records_kolom1,names_found1 = divide_in_records(kolom1,names)\n",
    "    records_kolom2,names_found2 = divide_in_records(kolom2,names)\n",
    "    \n",
    "    # check if all names in the header are found in the running text. If not, write to error log.\n",
    "        \n",
    "    names_found1.extend(names_found2)\n",
    "\n",
    "    if names[0] == last_name_of_previous_page:\n",
    "        names_to_spot = names[1:]\n",
    "    else:\n",
    "        names_to_spot = names\n",
    "   \n",
    "    names_not_found = list(set(names_to_spot)-set(names_found1))\n",
    "    \n",
    "    last_name_of_previous_page = names[-1]\n",
    "    \n",
    "    \n",
    "    if len(names_not_found) > 0:\n",
    "        opf_error.write(\"\\tFollowing names not found in page %s: %s\\n\"% (page_i+BEGIN-1,names_not_found))\n",
    "    records = merge_records(records_kolom1,records_kolom2,page_begin,page_end)\n",
    "    \n",
    "    if len(all_records)>0 and records[0].get(\"name\") == \"INHERIT\":\n",
    "        all_records[-1][\"text\"].extend(records[0][\"text\"])\n",
    "        all_records[-1][\"pages\"] +=  \"-%s\" % (page_begin)\n",
    "        if len(records) > 1:\n",
    "            all_records.extend(records[1:])\n",
    "    else:\n",
    "        all_records.extend(records)\n",
    "        \n",
    "# add record to list of records\n",
    "\n",
    "for r in all_records:\n",
    "    all_records_clean.append(merge_text(r))\n",
    "    \n",
    "# dump in json file\n",
    "\n",
    "db = json.dumps(all_records_clean,ensure_ascii=False).encode(\"utf-8\")\n",
    "\n",
    "opf = open(\"../output/vol_%s.json\"%(vol_nr),\"w\",encoding=\"utf-8\")\n",
    "json.dump(all_records_clean,opf,ensure_ascii=False,indent=4)\n",
    "opf.close()\n",
    "opf_error.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cf35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
